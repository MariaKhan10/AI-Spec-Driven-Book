"use strict";(globalThis.webpackChunkmy_speckitplus_practice=globalThis.webpackChunkmy_speckitplus_practice||[]).push([[579],{2952:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"robotics-applications/robotics-applications-chapter8-capstone-autonomous-humanoid","title":"Chapter 8: Capstone Project: The Autonomous Humanoid","description":"8.1 Introduction to the Capstone Project","source":"@site/docs/robotics-applications/chapter8-capstone-autonomous-humanoid.md","sourceDirName":"robotics-applications","slug":"/robotics-applications/robotics-applications-chapter8-capstone-autonomous-humanoid","permalink":"/AI-Spec-Driven-Book/docs/robotics-applications/robotics-applications-chapter8-capstone-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/MariaKhan10/AI-Spec-Driven-Book/edit/main/docs/robotics-applications/chapter8-capstone-autonomous-humanoid.md","tags":[],"version":"current","frontMatter":{"id":"robotics-applications-chapter8-capstone-autonomous-humanoid","title":"Chapter 8: Capstone Project: The Autonomous Humanoid"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: Conversational Robotics and Multimodal Interaction","permalink":"/AI-Spec-Driven-Book/docs/robotics-applications/robotics-applications-chapter7-conversational-robotics"}}');var o=i(4848),s=i(8453);const r={id:"robotics-applications-chapter8-capstone-autonomous-humanoid",title:"Chapter 8: Capstone Project: The Autonomous Humanoid"},a="Chapter 8: Capstone Project: The Autonomous Humanoid",l={},c=[{value:"8.1 Introduction to the Capstone Project",id:"81-introduction-to-the-capstone-project",level:2},{value:"Project Goal:",id:"project-goal",level:3},{value:"Integration of Core Modules:",id:"integration-of-core-modules",level:3},{value:"Project Stages and Milestones:",id:"project-stages-and-milestones",level:3},{value:"Required Software and Hardware Setup (referencing Chapter 9):",id:"required-software-and-hardware-setup-referencing-chapter-9",level:3},{value:"8.2 Project Setup and Simulation Environment",id:"82-project-setup-and-simulation-environment",level:2},{value:"Choosing the Simulation Environment: Gazebo vs. NVIDIA Isaac Sim",id:"choosing-the-simulation-environment-gazebo-vs-nvidia-isaac-sim",level:3},{value:"Robot Model Integration: Importing or Creating a Humanoid URDF/SDF Model",id:"robot-model-integration-importing-or-creating-a-humanoid-urdfsdf-model",level:3},{value:"Environment Design: Creating a Simple Indoor Environment with Obstacles and Target Objects",id:"environment-design-creating-a-simple-indoor-environment-with-obstacles-and-target-objects",level:3},{value:"Configuring ROS 2 Bridge for Communication between the Simulator and ROS 2 Nodes",id:"configuring-ros-2-bridge-for-communication-between-the-simulator-and-ros-2-nodes",level:3},{value:"8.3 Voice Command Interface (Module 4 &amp; Chapter 5 Integration)",id:"83-voice-command-interface-module-4--chapter-5-integration",level:2},{value:"Speech Recognition Implementation:",id:"speech-recognition-implementation",level:3},{value:"Natural Language Understanding for Command Interpretation:",id:"natural-language-understanding-for-command-interpretation",level:3},{value:"8.4 Cognitive Planning and Task Decomposition (Chapter 5 Integration)",id:"84-cognitive-planning-and-task-decomposition-chapter-5-integration",level:2},{value:"LLM-based Planner (or Rule-based System):",id:"llm-based-planner-or-rule-based-system",level:3},{value:"Action Primitive Library:",id:"action-primitive-library",level:3},{value:"Developing a ROS 2 Action Server to Execute the Planned Actions:",id:"developing-a-ros-2-action-server-to-execute-the-planned-actions",level:3},{value:"8.5 Navigation and Obstacle Avoidance (Module 3 &amp; Chapter 4 Integration)",id:"85-navigation-and-obstacle-avoidance-module-3--chapter-4-integration",level:2},{value:"Mapping: Creating a Map of the Simulated Environment",id:"mapping-creating-a-map-of-the-simulated-environment",level:3},{value:"Localization: Localizing the Robot within the Map",id:"localization-localizing-the-robot-within-the-map",level:3},{value:"Path Planning: Generating a Collision-Free Path",id:"path-planning-generating-a-collision-free-path",level:3},{value:"Dynamic Obstacle Avoidance:",id:"dynamic-obstacle-avoidance",level:3},{value:"8.6 Object Identification and Perception (Module 3 &amp; Chapter 4 Integration)",id:"86-object-identification-and-perception-module-3--chapter-4-integration",level:2},{value:"Sensor Data Acquisition:",id:"sensor-data-acquisition",level:3},{value:"Computer Vision Pipeline:",id:"computer-vision-pipeline",level:3},{value:"Publishing Object Information to ROS 2 Topics:",id:"publishing-object-information-to-ros-2-topics",level:3},{value:"8.7 Manipulation and Grasping (Chapter 6 Integration)",id:"87-manipulation-and-grasping-chapter-6-integration",level:2},{value:"Targeting the Object:",id:"targeting-the-object",level:3},{value:"Inverse Kinematics for Arm Control (Chapter 6 Integration):",id:"inverse-kinematics-for-arm-control-chapter-6-integration",level:3},{value:"Grasp Planning:",id:"grasp-planning",level:3},{value:"Executing the Grasp:",id:"executing-the-grasp",level:3},{value:"Carrying and Placing the Object:",id:"carrying-and-placing-the-object",level:3},{value:"8.8 Project Debugging, Testing, and Evaluation",id:"88-project-debugging-testing-and-evaluation",level:2},{value:"Debugging Techniques:",id:"debugging-techniques",level:3},{value:"Unit Testing:",id:"unit-testing",level:3},{value:"Integration Testing:",id:"integration-testing",level:3},{value:"Performance Evaluation:",id:"performance-evaluation",level:3},{value:"8.9 Extending the Capstone Project (Optional Advanced Topics)",id:"89-extending-the-capstone-project-optional-advanced-topics",level:2},{value:"Adding More Complex Environments and Tasks:",id:"adding-more-complex-environments-and-tasks",level:3},{value:"Implementing Adaptive Manipulation for Unknown Objects:",id:"implementing-adaptive-manipulation-for-unknown-objects",level:3},{value:"Integrating Force/Torque Sensing for Compliant Interaction:",id:"integrating-forcetorque-sensing-for-compliant-interaction",level:3},{value:"Exploring Human-like Gaze and Gesture During Interaction:",id:"exploring-human-like-gaze-and-gesture-during-interaction",level:3},{value:"Learning Outcomes for Chapter 8:",id:"learning-outcomes-for-chapter-8",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-8-capstone-project-the-autonomous-humanoid",children:"Chapter 8: Capstone Project: The Autonomous Humanoid"})}),"\n",(0,o.jsx)(n.h2,{id:"81-introduction-to-the-capstone-project",children:"8.1 Introduction to the Capstone Project"}),"\n",(0,o.jsx)(n.p,{children:"The Capstone Project serves as the culmination of the knowledge and skills acquired throughout this course. It challenges students to integrate various facets of Physical AI and Humanoid Robotics into a single, functional system. The project aims to bridge the theoretical understanding gained from previous chapters with practical implementation, providing a holistic experience in building an intelligent, embodied agent."}),"\n",(0,o.jsx)(n.h3,{id:"project-goal",children:"Project Goal:"}),"\n",(0,o.jsx)(n.p,{children:"Develop a simulated humanoid robot capable of:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Receiving a voice command:"})," Interpreting natural language instructions from a human user."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planning a path:"})," Generating a collision-free route to a target location."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigating obstacles:"})," Moving through a cluttered environment without collisions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Identifying an object:"})," Using computer vision to recognize and locate a specified object."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulating it:"})," Physically interacting with the object, such as picking it up and placing it."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"integration-of-core-modules",children:"Integration of Core Modules:"}),"\n",(0,o.jsx)(n.p,{children:"This project requires the seamless integration of several key components learned in earlier chapters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 (Chapter 2):"})," As the communication middleware for all robot components."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gazebo/Isaac Sim (Chapter 3 & 4):"})," For realistic physics simulation and environment modeling."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"NVIDIA Isaac AI (Chapter 4):"})," For advanced perception (VSLAM, object detection) and potentially advanced control."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"VLA (Vision-Language-Action) (Chapter 5):"})," For translating voice commands into actionable plans."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humanoid Control (Chapter 6):"})," For managing kinematics, dynamics, and bipedal locomotion."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"project-stages-and-milestones",children:"Project Stages and Milestones:"}),"\n",(0,o.jsx)(n.p,{children:"The project will be broken down into manageable stages, with clear milestones for each:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Setup and Environment:"})," Configure the simulation environment and robot model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Command Interface:"})," Implement speech recognition and natural language understanding."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning:"})," Develop the system to translate high-level commands into action sequences."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation Stack:"})," Implement mapping, localization, and path planning."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception and Object Identification:"})," Develop computer vision for object recognition and pose estimation."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation Control:"})," Implement grasping and object placement."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration and Testing:"})," Combine all modules and thoroughly test the system."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"required-software-and-hardware-setup-referencing-chapter-9",children:"Required Software and Hardware Setup (referencing Chapter 9):"}),"\n",(0,o.jsx)(n.p,{children:'Students will utilize the recommended hardware and software stack from Chapter 9, primarily focusing on a powerful "Digital Twin" Workstation running Ubuntu with ROS 2 and either Gazebo or NVIDIA Isaac Sim. Familiarity with the installation and configuration of these tools is assumed.'}),"\n",(0,o.jsx)(n.h2,{id:"82-project-setup-and-simulation-environment",children:"8.2 Project Setup and Simulation Environment"}),"\n",(0,o.jsx)(n.p,{children:"The first critical step for the Capstone Project is setting up the simulation environment and integrating the humanoid robot model. A well-configured simulation provides a safe and efficient platform for developing and testing complex robotic behaviors."}),"\n",(0,o.jsx)(n.h3,{id:"choosing-the-simulation-environment-gazebo-vs-nvidia-isaac-sim",children:"Choosing the Simulation Environment: Gazebo vs. NVIDIA Isaac Sim"}),"\n",(0,o.jsx)(n.p,{children:"Both Gazebo and NVIDIA Isaac Sim offer powerful simulation capabilities, but they have different strengths. Students should choose based on their available computational resources and specific project needs."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gazebo (Chapter 3):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pros:"})," Open-source, widely adopted, deep integration with ROS 2, good for basic physics and sensor simulation. Lower hardware requirements compared to Isaac Sim."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cons:"})," Less photorealistic rendering, may be less optimized for large-scale synthetic data generation compared to Isaac Sim."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"NVIDIA Isaac Sim (Chapter 4):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pros:"})," High-fidelity, physically accurate, photorealistic rendering (requires RTX GPU), excellent for synthetic data generation and AI training, strong integration with Isaac ROS."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cons:"})," Higher hardware requirements (RTX GPU mandatory), can be more complex to set up initially."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"For this Capstone, it's recommended to start with Gazebo if hardware is a constraint, or leverage Isaac Sim if an RTX workstation is available to benefit from its advanced features."}),"\n",(0,o.jsx)(n.h3,{id:"robot-model-integration-importing-or-creating-a-humanoid-urdfsdf-model",children:"Robot Model Integration: Importing or Creating a Humanoid URDF/SDF Model"}),"\n",(0,o.jsx)(n.p,{children:"The humanoid robot model is the central component of the simulation."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Using Existing Models:"})," Many humanoid robot models (e.g., in URDF format) are available online or come with ROS 2 packages. Students can import a suitable model into their chosen simulator."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Creating Custom Models (Advanced):"})," For those seeking a deeper challenge, creating a custom humanoid URDF/SDF model (Chapter 2 & 3) allows for specific design choices and understanding of robot kinematics."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["For Gazebo: Ensure the URDF model includes ",(0,o.jsx)(n.code,{children:"gazebo"})," tags for physics properties and sensor plugins."]}),"\n",(0,o.jsx)(n.li,{children:"For Isaac Sim: Models are typically loaded as USD assets; URDF can often be converted or imported."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"environment-design-creating-a-simple-indoor-environment-with-obstacles-and-target-objects",children:"Environment Design: Creating a Simple Indoor Environment with Obstacles and Target Objects"}),"\n",(0,o.jsx)(n.p,{children:"A simple, yet functional, environment is needed for the robot to operate in."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scenario:"})," A basic room-like setting with a floor, walls, and a table."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Obstacles:"})," Introduce static obstacles (e.g., boxes, chairs) that the robot must navigate around."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Target Objects:"})," Place a distinct object (e.g., a colored block, a cup) on the table for the robot to identify and manipulate."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulator Tools:"})," Utilize the built-in world editors of Gazebo or Isaac Sim to create and populate the environment."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"configuring-ros-2-bridge-for-communication-between-the-simulator-and-ros-2-nodes",children:"Configuring ROS 2 Bridge for Communication between the Simulator and ROS 2 Nodes"}),"\n",(0,o.jsx)(n.p,{children:"Regardless of the chosen simulator, a bridge is required for communication with the ROS 2 ecosystem."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsxs)(n.strong,{children:[(0,o.jsx)(n.code,{children:"gazebo_ros_pkgs"})," (for Gazebo):"]})," Provides the necessary plugins and nodes to bridge Gazebo with ROS 2, allowing simulated sensor data to be published to ROS 2 topics and ROS 2 commands to control the simulated robot."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac Sim ROS 2 Bridge (for Isaac Sim):"})," Isaac Sim has its own ROS 2 bridge to facilitate similar communication."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Verification:"})," Ensure that publishing/subscribing to basic topics (e.g., joint states, ",(0,o.jsx)(n.code,{children:"/cmd_vel"}),") works correctly between your ROS 2 nodes and the simulated robot."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"83-voice-command-interface-module-4--chapter-5-integration",children:"8.3 Voice Command Interface (Module 4 & Chapter 5 Integration)"}),"\n",(0,o.jsx)(n.p,{children:"The ability for the humanoid robot to understand spoken commands is a cornerstone of this Capstone Project, integrating concepts from Module 4 and Chapter 5. This interface translates human voice into structured commands that the robot can process."}),"\n",(0,o.jsx)(n.h3,{id:"speech-recognition-implementation",children:"Speech Recognition Implementation:"}),"\n",(0,o.jsx)(n.p,{children:"The first step is to convert human speech into text."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Setting up OpenAI Whisper (or an alternative ASR):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Install and configure OpenAI Whisper (or an open-source alternative like ",(0,o.jsx)(n.code,{children:"Vosk"})," or ",(0,o.jsx)(n.code,{children:"Mozilla DeepSpeech"})," if Whisper is not feasible or for learning purposes)."]}),"\n",(0,o.jsx)(n.li,{children:"Ensure proper audio input capture from the workstation's microphone."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Developing a ROS 2 Node for Audio Capture and Transcription:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Create a Python ROS 2 node (e.g., ",(0,o.jsx)(n.code,{children:"voice_listener_node"}),") that:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Accesses the microphone to capture audio streams."}),"\n",(0,o.jsx)(n.li,{children:"Feeds the audio data to the chosen ASR system (e.g., Whisper)."}),"\n",(0,o.jsxs)(n.li,{children:["Publishes the resulting text transcript to a ROS 2 topic (e.g., ",(0,o.jsx)(n.code,{children:"/speech_text"}),")."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Example Code Snippet (Conceptual):"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# speech_listener_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper # Assuming Whisper is installed\r\n\r\nclass SpeechListener(Node):\r\n    def __init__(self):\r\n        super().__init__('speech_listener')\r\n        self.publisher_ = self.create_publisher(String, 'speech_text', 10)\r\n        self.model = whisper.load_model(\"base\") # Load a Whisper model\r\n        self.get_logger().info('Speech Listener Node started, awaiting commands...')\r\n        # Placeholder for actual audio capture and transcription loop\r\n        # In a real system, this would involve continuous audio capture and processing\r\n\r\n    def process_audio(self, audio_data):\r\n        # This function would be called when audio is available\r\n        # For simplicity, let's assume `audio_data` is already preprocessed for Whisper\r\n        result = self.model.transcribe(audio_data)\r\n        transcribed_text = result[\"text\"]\r\n        msg = String()\r\n        msg.data = transcribed_text\r\n        self.publisher_.publish(msg)\r\n        self.get_logger().info(f'Transcribed: \"{transcribed_text}\"')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    speech_listener = SpeechListener()\r\n    rclpy.spin(speech_listener) # Keep node alive\r\n    speech_listener.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"natural-language-understanding-for-command-interpretation",children:"Natural Language Understanding for Command Interpretation:"}),"\n",(0,o.jsx)(n.p,{children:"The transcribed text needs to be understood in the context of robot actions."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Using an LLM (or a simplified rule-based system):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["For the Capstone, you can choose between:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"A simplified rule-based system:"}),' Using keywords, regular expressions, and predefined patterns to extract intent and entities (e.g., "move forward 2 meters" -> ',(0,o.jsx)(n.code,{children:"intent: move"}),", ",(0,o.jsx)(n.code,{children:"direction: forward"}),", ",(0,o.jsx)(n.code,{children:"distance: 2"}),"). This is easier to implement for a Capstone."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"A local, smaller LLM (e.g., through Hugging Face Transformers):"})," If computational resources allow, a smaller LLM can be fine-tuned or prompted to act as an NLU engine, offering more flexibility."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Developing a ROS 2 Node for NLU:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Create a Python ROS 2 node (e.g., ",(0,o.jsx)(n.code,{children:"command_parser_node"}),") that:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Subscribes to the ",(0,o.jsx)(n.code,{children:"/speech_text"})," topic."]}),"\n",(0,o.jsxs)(n.li,{children:["Applies the NLU logic to extract the robot's ",(0,o.jsx)(n.code,{children:"intent"})," and ",(0,o.jsx)(n.code,{children:"entities"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:["Publish a structured command (e.g., a custom ROS 2 message type like ",(0,o.jsx)(n.code,{children:"robot_interfaces/msg/RobotCommand"}),") to a new topic (e.g., ",(0,o.jsx)(n.code,{children:"/robot_commands"}),")."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Example Structured Command (Custom ROS 2 Message Concept):"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-rosidl",children:'# robot_interfaces/msg/RobotCommand.msg\r\nstring intent        # e.g., "move", "grasp", "identify", "plan"\r\nstring[] entities    # e.g., ["direction:forward", "distance:2.0", "object:red_block"]\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This structured command will then be the input for the Cognitive Planning module."}),"\n",(0,o.jsx)(n.h2,{id:"84-cognitive-planning-and-task-decomposition-chapter-5-integration",children:"8.4 Cognitive Planning and Task Decomposition (Chapter 5 Integration)"}),"\n",(0,o.jsxs)(n.p,{children:["With a structured command received from the Voice Command Interface, the robot needs to formulate a detailed plan of action. This ",(0,o.jsx)(n.strong,{children:"Cognitive Planning"})," module, heavily drawing from Chapter 5, translates the high-level intent into a sequence of executable robot primitives."]}),"\n",(0,o.jsx)(n.h3,{id:"llm-based-planner-or-rule-based-system",children:"LLM-based Planner (or Rule-based System):"}),"\n",(0,o.jsx)(n.p,{children:"The core of this module is deciding how to interpret and sequence actions."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM-based Planner (Advanced):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"If using an LLM, it would receive the structured command and the current environmental state (e.g., list of perceived objects, robot's location)."}),"\n",(0,o.jsxs)(n.li,{children:["The LLM, through careful prompt engineering, would then generate a sequence of ",(0,o.jsx)(n.em,{children:"robot primitive actions"})," (defined below)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Example Prompt (Conceptual):"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:'"You are a humanoid robot. Current state: Robot at (0,0,0), Red block at (1,1,0), Blue block at (2,0,0).\r\nAvailable actions:\r\n- move_to(x,y,z)\r\n- grasp(object_name)\r\n- place_object_at(object_name, x,y,z)\r\n- identify_object_in_view()\r\n\r\nCommand: "Pick up the red block and put it on the blue block."\r\nGenerate a plan using the available actions."\n'})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Expected LLM Output (Conceptual):"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'[\r\n  {"action": "move_to", "args": {"x": 1, "y": 1, "z": 0}},\r\n  {"action": "grasp", "args": {"object_name": "red_block"}},\r\n  {"action": "move_to", "args": {"x": 2, "y": 0, "z": 0}},\r\n  {"action": "place_object_at", "args": {"object_name": "red_block", "x": 2, "y": 0, "z": 0}}\r\n]\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Rule-based Task Decomposer (Recommended for Capstone):"})," For a Capstone, a simpler rule-based system or state machine is often more manageable.","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"It would define a set of rules to break down intents into a fixed sequence of primitive actions."}),"\n",(0,o.jsxs)(n.li,{children:["Example: If ",(0,o.jsx)(n.code,{children:'intent: "pick_and_place"'}),", then sequence: ",(0,o.jsx)(n.code,{children:"move_to_object"}),", ",(0,o.jsx)(n.code,{children:"identify_object"}),", ",(0,o.jsx)(n.code,{children:"grasp_object"}),", ",(0,o.jsx)(n.code,{children:"move_to_target"}),", ",(0,o.jsx)(n.code,{children:"place_object"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"action-primitive-library",children:"Action Primitive Library:"}),"\n",(0,o.jsxs)(n.p,{children:["A crucial component is a well-defined set of ",(0,o.jsx)(n.strong,{children:"action primitives"})," \u2013 the low-level, robust behaviors that the robot can reliably execute. Each primitive should correspond to a ROS 2 interface (service, action, or topic)."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Examples of Action Primitives:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"move_base_to_pose(x, y, yaw)"})," (ROS 2 Action for navigation)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"look_at_point(x, y, z)"})," (ROS 2 Service to control head/camera)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"perform_grasp(object_id)"})," (ROS 2 Action for manipulation)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"release_object()"})," (ROS 2 Service)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"get_object_info(object_name)"})," (ROS 2 Service for perception query)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"developing-a-ros-2-action-server-to-execute-the-planned-actions",children:"Developing a ROS 2 Action Server to Execute the Planned Actions:"}),"\n",(0,o.jsx)(n.p,{children:"The Cognitive Planner will typically manage the execution of these primitives."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Create a ROS 2 Action Server (e.g., ",(0,o.jsx)(n.code,{children:"task_execution_server"}),") that:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Receives the high-level plan (sequence of primitive actions) as a goal."}),"\n",(0,o.jsx)(n.li,{children:"Executes each primitive action sequentially, by calling the corresponding ROS 2 services/actions of other robot modules (navigation, perception, manipulation)."}),"\n",(0,o.jsx)(n.li,{children:'Sends feedback during execution (e.g., "Executing move_to step 1/4").'}),"\n",(0,o.jsx)(n.li,{children:"Returns success or failure upon plan completion."}),"\n",(0,o.jsx)(n.li,{children:"Handles errors or failures of individual primitive actions, potentially triggering re-planning or notifying the human."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This ",(0,o.jsx)(n.code,{children:"task_execution_server"})," acts as the orchestrator, ensuring the LLM-generated (or rule-based) plan is translated into physical robot movements."]}),"\n",(0,o.jsx)(n.h2,{id:"85-navigation-and-obstacle-avoidance-module-3--chapter-4-integration",children:"8.5 Navigation and Obstacle Avoidance (Module 3 & Chapter 4 Integration)"}),"\n",(0,o.jsxs)(n.p,{children:["For the autonomous humanoid to move effectively within its environment, robust ",(0,o.jsx)(n.strong,{children:"navigation and obstacle avoidance"})," capabilities are essential. This module integrates concepts from Gazebo simulation (Chapter 3) and NVIDIA Isaac (Chapter 4), particularly Nav2."]}),"\n",(0,o.jsx)(n.h3,{id:"mapping-creating-a-map-of-the-simulated-environment",children:"Mapping: Creating a Map of the Simulated Environment"}),"\n",(0,o.jsx)(n.p,{children:"Before navigation, the robot needs a map of its surroundings."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"SLAM (Simultaneous Localization and Mapping):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Using simulated LIDAR data (from Gazebo/Isaac Sim) and odometry."}),"\n",(0,o.jsxs)(n.li,{children:["Implement a ROS 2 SLAM algorithm (e.g., ",(0,o.jsx)(n.code,{children:"slam_toolbox"})," or ",(0,o.jsx)(n.code,{children:"cartographer"}),") to build an occupancy grid map of the environment."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS VSLAM (Advanced):"})," If using Isaac Sim and an RTX GPU, leverage Isaac ROS VSLAM (Chapter 4) for visual-inertial SLAM, providing a high-performance mapping solution."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pre-built Map:"})," For simplicity in a Capstone, a static, pre-built map of the simulated environment can also be loaded."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"localization-localizing-the-robot-within-the-map",children:"Localization: Localizing the Robot within the Map"}),"\n",(0,o.jsx)(n.p,{children:"The robot needs to know where it is on the map."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"AMCL (Adaptive Monte Carlo Localization):"})," A probabilistic localization algorithm widely used in ROS 2. It takes LIDAR scans and odometry data to estimate the robot's pose on a known map."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS Localization (Advanced):"})," Integrate Isaac ROS localization packages for GPU-accelerated and potentially more robust localization, especially with visual inputs."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"path-planning-generating-a-collision-free-path",children:"Path Planning: Generating a Collision-Free Path"}),"\n",(0,o.jsx)(n.p,{children:"Once localized, the robot needs to plan a path to its target."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Nav2 (ROS 2 Navigation Stack):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Utilize Nav2 for global and local path planning."}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Global Planner:"})," Generates a high-level, long-term path from the robot's current location to the target."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Local Planner:"})," Responsible for dynamic obstacle avoidance and generating short-term velocity commands to follow the global path."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Bipedal Locomotion Control (Chapter 6 Integration):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Since Nav2 is typically for wheeled robots, the output (velocity commands or poses) needs to be adapted for bipedal movement."}),"\n",(0,o.jsx)(n.li,{children:"The bipedal gait controller (Chapter 6) will take these planned velocities/poses and translate them into stable walking patterns."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Target Location:"}),' The target location will be provided by the Cognitive Planning module based on the human\'s command (e.g., "go to the table").']}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"dynamic-obstacle-avoidance",children:"Dynamic Obstacle Avoidance:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The local planner within Nav2 continuously monitors sensor data (LIDAR, depth cameras) to detect dynamic obstacles and adjust the robot's path in real-time to avoid collisions. This ensures safety during movement."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"86-object-identification-and-perception-module-3--chapter-4-integration",children:"8.6 Object Identification and Perception (Module 3 & Chapter 4 Integration)"}),"\n",(0,o.jsxs)(n.p,{children:["For the autonomous humanoid to perform manipulation tasks, it must first be able to ",(0,o.jsx)(n.strong,{children:"identify and locate specific objects"})," in its environment. This module integrates concepts from sensor simulation (Chapter 3) and advanced AI perception (Chapter 4)."]}),"\n",(0,o.jsx)(n.h3,{id:"sensor-data-acquisition",children:"Sensor Data Acquisition:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulated Camera Data:"})," Utilize simulated RGB-D (color and depth) camera data published from Gazebo or Isaac Sim (Chapter 3). This data will mimic what a real Intel RealSense camera would provide."]}),"\n",(0,o.jsxs)(n.li,{children:["Ensure that the camera ROS 2 topic (e.g., ",(0,o.jsx)(n.code,{children:"/camera/image_raw"}),", ",(0,o.jsx)(n.code,{children:"/camera/depth/image_raw"}),") is accessible to the perception node."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"computer-vision-pipeline",children:"Computer Vision Pipeline:"}),"\n",(0,o.jsx)(n.p,{children:"The core of this module is a computer vision pipeline that processes camera data to identify and localize objects."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Object Detection:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Goal:"}),' Identify the target object (e.g., "red block") specified in the human command.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Techniques:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pre-trained Deep Learning Models:"})," For robust detection, use or adapt a pre-trained object detection model (e.g., YOLO, SSD, Faster R-CNN) if computational resources (GPU) allow (Chapter 4). This could be deployed via Isaac ROS for acceleration."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Custom Model with Synthetic Data:"})," Train a custom, lightweight object detection model using synthetic data generated from Isaac Sim (Chapter 4) for the specific objects in the Capstone environment."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simple Color/Shape Detection (Recommended for Capstone):"})," For simplicity, a rule-based approach using OpenCV for color thresholding and contour detection can identify basic colored blocks."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"The output of detection would be bounding box coordinates in the 2D image."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"**Pose Estimation (Determining 3D Position and Orientation):"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Once an object is detected in 2D, its 3D pose (position and orientation) relative to the robot's camera or base frame needs to be estimated."}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Using Depth Data:"})," Combine the 2D bounding box with the depth map from the RGB-D camera to estimate the 3D coordinates of the object."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"PnP (Perspective-n-Point) Algorithm:"})," For more accurate pose estimation, if a 3D model of the object is known, algorithms like PnP can be used to estimate the 6-DoF pose."]}),"\n",(0,o.jsxs)(n.li,{children:["The output would be the object's ",(0,o.jsx)(n.code,{children:"geometry_msgs/msg/PoseStamped"})," message."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"publishing-object-information-to-ros-2-topics",children:"Publishing Object Information to ROS 2 Topics:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Create a ROS 2 node (e.g., ",(0,o.jsx)(n.code,{children:"object_perception_node"}),") that:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Subscribes to the camera image and depth topics."}),"\n",(0,o.jsx)(n.li,{children:"Runs the object detection and pose estimation pipeline."}),"\n",(0,o.jsxs)(n.li,{children:["Publishes the detected objects' information (e.g., ",(0,o.jsx)(n.code,{children:"object_id"}),", ",(0,o.jsx)(n.code,{children:"pose"}),") to a ROS 2 topic (e.g., ",(0,o.jsx)(n.code,{children:"/perceived_objects"}),"). This topic will be subscribed to by the manipulation module."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"87-manipulation-and-grasping-chapter-6-integration",children:"8.7 Manipulation and Grasping (Chapter 6 Integration)"}),"\n",(0,o.jsxs)(n.p,{children:["With the target object identified and located in 3D space, the humanoid robot must now execute a ",(0,o.jsx)(n.strong,{children:"manipulation task"}),", typically involving grasping and placement. This module heavily relies on the kinematics and control principles discussed in Chapter 6."]}),"\n",(0,o.jsx)(n.h3,{id:"targeting-the-object",children:"Targeting the Object:"}),"\n",(0,o.jsx)(n.p,{children:"The first step is to establish the target for the robot's end-effector (hand)."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Desired End-Effector Pose:"})," Using the perceived object pose (from the perception module), determine a suitable ",(0,o.jsx)(n.strong,{children:"pre-grasp pose"})," (approaching the object) and a ",(0,o.jsx)(n.strong,{children:"grasp pose"})," (where the hand needs to be to grasp the object). This involves considering the hand geometry and the object's shape."]}),"\n",(0,o.jsxs)(n.li,{children:["The ",(0,o.jsx)(n.code,{children:"object_perception_node"})," might directly publish this desired hand pose, or the manipulation node calculates it based on the object's pose."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"inverse-kinematics-for-arm-control-chapter-6-integration",children:"Inverse Kinematics for Arm Control (Chapter 6 Integration):"}),"\n",(0,o.jsx)(n.p,{children:"Once the desired hand pose is known, the robot needs to determine the joint angles required to achieve it."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Inverse Kinematics (IK) Solver:"})," Implement or integrate an IK solver (Chapter 6) for the humanoid's arm.","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The IK solver takes the desired end-effector pose (position and orientation) as input."}),"\n",(0,o.jsx)(n.li,{children:"It outputs a set of joint angles for the robot's arm (e.g., shoulder, elbow, wrist joints)."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Collision Avoidance in IK:"})," Ensure the IK solver considers joint limits and avoids self-collisions or collisions with the environment during the motion."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Trajectory Generation:"})," Generate a smooth trajectory of joint angles from the robot's current arm configuration to the pre-grasp and then grasp configuration."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"grasp-planning",children:"Grasp Planning:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Determining Grasp Configuration:"})," For basic objects like blocks, a simple parallel-jaw grasp might suffice. For more complex objects, the grasp planning module determines which fingers to close and with what force."]}),"\n",(0,o.jsx)(n.li,{children:"This can be a pre-programmed strategy for the Capstone or a more advanced learning-based approach if desired."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"executing-the-grasp",children:"Executing the Grasp:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sending Joint Commands:"})," Once the joint angles for the grasp pose are computed, send these commands to the simulated robot's arm and hand controllers via ROS 2 topics or action goals (e.g., ",(0,o.jsx)(n.code,{children:"control_msgs/msg/JointTrajectory"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Closing the Hand:"})," Control the gripper or fingers of the humanoid hand to close around the object."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Lifting the Object:"})," After grasping, generate a small upward motion to lift the object clear of the surface."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"carrying-and-placing-the-object",children:"Carrying and Placing the Object:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Move to Target Location:"}),' The Cognitive Planning module will provide the target location for placing the object (e.g., "put it on the blue block").']}),"\n",(0,o.jsx)(n.li,{children:"Use the Navigation module (and bipedal locomotion) to move the robot while carrying the object."}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Release Object:"})," Once at the target placement location, generate a pre-place pose, open the hand, and then move away."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"88-project-debugging-testing-and-evaluation",children:"8.8 Project Debugging, Testing, and Evaluation"}),"\n",(0,o.jsx)(n.p,{children:"A complex project like the Autonomous Humanoid Capstone requires thorough debugging, testing, and systematic evaluation to ensure functionality, robustness, and performance."}),"\n",(0,o.jsx)(n.h3,{id:"debugging-techniques",children:"Debugging Techniques:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 Logging:"})," Utilize ",(0,o.jsx)(n.code,{children:"rclpy.logging"})," (Python) or ",(0,o.jsx)(n.code,{children:"rclcpp::Logger"})," (C++) to print informative messages, warnings, and errors from your nodes."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsxs)(n.strong,{children:[(0,o.jsx)(n.code,{children:"rqt"})," Tools:"]})," A suite of GUI tools for ROS 2:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"rqt_graph"}),": Visualize the node and topic graph to check communication flow."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"rqt_console"}),": View ROS 2 log messages."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"rqt_plot"}),": Plot data from ROS 2 topics (e.g., joint angles, IMU readings)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"rqt_image_view"}),": View camera feeds."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulator Visualizations:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Gazebo GUI: Observe robot movements, collisions, and sensor outputs in real-time."}),"\n",(0,o.jsx)(n.li,{children:"RViz2: A 3D visualization tool for ROS 2 data. Display robot model, point clouds, maps, planned paths, and object poses."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Step-by-Step Execution:"})," Isolate individual modules and test them in isolation before integrating them."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"unit-testing",children:"Unit Testing:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Purpose:"})," Verify that individual components or functions work correctly in isolation."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Examples:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"ASR module: Test accuracy of transcription for various voice commands."}),"\n",(0,o.jsx)(n.li,{children:"NLU module: Test correct extraction of intent and entities from text."}),"\n",(0,o.jsx)(n.li,{children:"IK solver: Test if it returns correct joint angles for desired poses."}),"\n",(0,o.jsx)(n.li,{children:"Perception module: Test object detection accuracy and pose estimation given simulated images."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Use standard Python testing frameworks (e.g., ",(0,o.jsx)(n.code,{children:"unittest"}),", ",(0,o.jsx)(n.code,{children:"pytest"}),")."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"integration-testing",children:"Integration Testing:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Purpose:"})," Verify the seamless interaction and communication between different modules."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Examples:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Voice command to navigation: Speak a "go to" command and observe if the robot correctly plans and executes the path.'}),"\n",(0,o.jsx)(n.li,{children:"Perception to manipulation: Verify that a detected object's pose is correctly used by the manipulation module for grasping."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"This often involves setting up specific scenarios in the simulator."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"performance-evaluation",children:"Performance Evaluation:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robot Responsiveness:"})," Measure the latency between a command being issued and the robot beginning to respond."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Success Rate:"})," For each task (navigation, grasping, full capstone task), record the percentage of successful attempts."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Efficiency:"})," Evaluate metrics like completion time, path length, and energy consumption (if models allow) for tasks."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness:"})," Test the system under varying conditions (e.g., noisy environment, slight object misplacement, unexpected obstacles)."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"89-extending-the-capstone-project-optional-advanced-topics",children:"8.9 Extending the Capstone Project (Optional Advanced Topics)"}),"\n",(0,o.jsx)(n.p,{children:"For students who wish to delve deeper and further challenge themselves, the Capstone Project can be extended with several advanced topics."}),"\n",(0,o.jsx)(n.h3,{id:"adding-more-complex-environments-and-tasks",children:"Adding More Complex Environments and Tasks:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dynamic Environments:"})," Introduce moving obstacles or changing lighting conditions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-room Navigation:"})," Navigate between multiple rooms with doors that might need to be opened."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Complex Manipulation:"})," Tasks requiring tools, or manipulation of deformable/fragile objects."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementing-adaptive-manipulation-for-unknown-objects",children:"Implementing Adaptive Manipulation for Unknown Objects:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Move beyond pre-programmed grasps for known objects."}),"\n",(0,o.jsx)(n.li,{children:"Explore learning-based grasping techniques that can adapt to novel object geometries using vision and tactile sensing."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"integrating-forcetorque-sensing-for-compliant-interaction",children:"Integrating Force/Torque Sensing for Compliant Interaction:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Incorporate force/torque sensors (Chapter 1) into the robot's wrists or fingertips."}),"\n",(0,o.jsx)(n.li,{children:'Implement compliance control to allow the robot to "give" when it encounters resistance, crucial for safe human-robot collaboration or delicate tasks.'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"exploring-human-like-gaze-and-gesture-during-interaction",children:"Exploring Human-like Gaze and Gesture During Interaction:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Enhance the HRI (Chapter 6 & 7) by enabling the robot to make eye contact (or orient its head/camera) with the human speaker."}),"\n",(0,o.jsx)(n.li,{children:"Implement robot gestures (e.g., pointing, nodding) to enrich communication and clarify intent, making the interaction feel more natural and intuitive."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes-for-chapter-8",children:"Learning Outcomes for Chapter 8:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Design and implement an end-to-end autonomous humanoid robot system in simulation."}),"\n",(0,o.jsx)(n.li,{children:"Integrate speech recognition, natural language understanding, and cognitive planning for voice-command control."}),"\n",(0,o.jsx)(n.li,{children:"Apply navigation algorithms for path planning and obstacle avoidance in a humanoid context."}),"\n",(0,o.jsx)(n.li,{children:"Develop computer vision pipelines for object identification and pose estimation."}),"\n",(0,o.jsx)(n.li,{children:"Implement manipulation and grasping capabilities for humanoid robots."}),"\n",(0,o.jsx)(n.li,{children:"Debug, test, and evaluate complex robotic systems effectively."}),"\n",(0,o.jsx)(n.li,{children:"Synthesize knowledge from all previous chapters into a functional robotic application."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);